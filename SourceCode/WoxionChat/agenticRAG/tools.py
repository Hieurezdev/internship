import os
import logging
import re
from typing import List, Dict, Any
from langchain.agents import tool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from .db import (
    find_similar_documents_hybrid_search, 
    find_similar_documents_vector_search,
    get_embedding
)
from google import genai
from google.genai import types
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

logger = logging.getLogger(__name__)

_safety_settings = [
    types.SafetySetting(category='HARM_CATEGORY_HARASSMENT',        threshold='BLOCK_MEDIUM_AND_ABOVE'),
    types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH',       threshold='BLOCK_MEDIUM_AND_ABOVE'),
    types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
    types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_ONLY_HIGH'),
]

_generation_config = types.GenerateContentConfig(
    temperature=0.1,
    top_p=0.95,
    safety_settings=_safety_settings,
)

_genai_client: genai.Client | None = None

def _get_model_client() -> genai.Client:
    """Returns a lazily initialised google-genai Client."""
    global _genai_client
    if _genai_client is None:
        api_key = os.environ.get('GOOGLE_API_KEY') or os.environ.get('google_api_key')
        _genai_client = genai.Client(api_key=api_key)
    return _genai_client


def safe_log_info(message: str):
    """
    Safely log info messages with Unicode characters.
    Falls back to ASCII representation if encoding fails.
    """
    try:
        logger.info(message)
    except UnicodeEncodeError:
        # Fallback: Replace non-ASCII characters with ASCII equivalents
        safe_message = message.encode('ascii', 'replace').decode('ascii')
        logger.info(f"[UNICODE_SAFE] {safe_message}")

def safe_log_warning(message: str):
    """
    Safely log warning messages with Unicode characters.
    Falls back to ASCII representation if encoding fails.
    """
    try:
        logger.warning(message)
    except UnicodeEncodeError:
        # Fallback: Replace non-ASCII characters with ASCII equivalents
        safe_message = message.encode('ascii', 'replace').decode('ascii')
        logger.warning(f"[UNICODE_SAFE] {safe_message}")

def safe_log_error(message: str, exc_info=None):
    """
    Safely log error messages with Unicode characters.
    Falls back to ASCII representation if encoding fails.
    """
    try:
        logger.error(message, exc_info=exc_info)
    except UnicodeEncodeError:
        # Fallback: Replace non-ASCII characters with ASCII equivalents
        safe_message = message.encode('ascii', 'replace').decode('ascii')
        logger.error(f"[UNICODE_SAFE] {safe_message}", exc_info=exc_info)


@tool
def summarize_conversation(messages: List[str], user_preferences: Dict[str, Any] = None) -> str:
    """
    TÃ³m táº¯t cuá»™c há»™i thoáº¡i sá»­ dá»¥ng Gemini API.
    
    Args:
        messages: Danh sÃ¡ch tin nháº¯n trong cuá»™c há»™i thoáº¡i
        user_preferences: ThÃ´ng tin preferences cá»§a user (optional)
        
    Returns:
        Summary cá»§a cuá»™c há»™i thoáº¡i
    """
    safe_log_info("--- Running Tool: summarize_conversation ---")
    
    try:
        if not messages or len(messages) == 0:
            return "KhÃ´ng cÃ³ tin nháº¯n Ä‘á»ƒ tÃ³m táº¯t"
        
        # Simple conversation text
        conversation_text = "\n".join(messages[-15:])  
        
        # Simple prompt
        prompt = f"""HÃ£y tÃ³m táº¯t cuá»™c há»™i thoáº¡i sau trong 3-4 cÃ¢u ngáº¯n gá»n:\n\n{conversation_text}
            Giá»¯ láº¡i nhá»¯ng Ã½ chÃ­nh vÃ  cÃ¡c thÃ´ng tin quan trá»ng.
            giá»¯ láº¡i nhá»¯ng kiáº¿n thá»©c source cá»§a ngÆ°á»i dÃ¹ng (vÃ­ dá»¥: tÃªn tÃ i khoáº£n, tÃªn tÃ i khoáº£n cá»§a ngÆ°á»i dÃ¹ng Ä‘Ã£ táº£i lÃªn, ...)
            LÆ°u trá»¯ thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng (náº¿u cÃ³) (vÃ­ dá»¥: TÃªn, tuá»•i, giá»›i tÃ­nh, email, sá»‘ Ä‘iá»‡n thoáº¡i, Ä‘á»‹a chá»‰, ...)
            Äá»ƒ tÃ³m táº¯t cuá»™c há»™i thoáº¡i, hÃ£y ngáº¯n gá»n vÃ  dá»… hiá»ƒu.
        """
        
        # Call Gemini API using sync
        response = _get_model_client().models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=_generation_config
        )
        summary = response.text.strip()
        
        safe_log_info(f"Generated summary: {summary[:100]}...")
        return summary
        
    except Exception as e:
        safe_log_error(f"Error in summarize_conversation: {e}")
        return "Cuá»™c há»™i thoáº¡i vá» cÃ¡c chá»§ Ä‘á» cÃ´ng nghá»‡" 

@tool
def find_documents_parallel(search_query: str, uploader_username: str) -> Dict[str, List[dict]]:
    """
    TÃ¬m kiáº¿m tÃ i liá»‡u tá»« cáº£ user vÃ  admin song song Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™ vá»›i shared embedding
    """
    safe_log_info("--- Running Tool: find_documents_parallel ---")
    safe_log_info(f"Searching documents in parallel for query: '{search_query}' and uploader: '{uploader_username}'")
    
    # Generate embedding once for both searches
    start_time = time.time()
    query_vector = get_embedding(search_query)
    if not query_vector:
        logger.error("Failed to generate embedding for parallel document search")
        return {
            "user_documents": [],
            "admin_documents": []
        }
    
    embedding_time = time.time() - start_time
    safe_log_info(f"Embedding generated in {embedding_time:.3f}s")
    
    def get_user_documents():
        """Retrieve user documents using pre-generated embedding."""
        try:
            documents = find_similar_documents_hybrid_search(query_vector, search_query, uploader_username)
            safe_log_info(f"Found {len(documents)} user documents")
            return documents
        except Exception as e:
            safe_log_error(f"Error in user document search: {e}", exc_info=True)
            return []
    
    def get_admin_documents():
        """Retrieve admin documents using pre-generated embedding."""
        try:
            documents = find_similar_documents_vector_search(query_vector)
            safe_log_info(f"Found {len(documents)} admin documents")
            return documents
        except Exception as e:
            safe_log_error(f"Error in admin document search: {e}", exc_info=True)
            return []
    
    try:
        search_start = time.time()
        
        # Run both searches in parallel with shared embedding
        with ThreadPoolExecutor(max_workers=2) as executor:
            user_future = executor.submit(get_user_documents)
            admin_future = executor.submit(get_admin_documents)
            
            user_docs = []
            admin_docs = []
            
            # Collect results as they complete
            for future in as_completed([user_future, admin_future]):
                try:
                    if future == user_future:
                        user_docs = future.result()
                    elif future == admin_future:
                        admin_docs = future.result()
                except Exception as e:
                    safe_log_error(f"Error in parallel document retrieval: {e}")
        
        search_time = time.time() - search_start
        total_time = time.time() - start_time
        safe_log_info(f"Parallel document retrieval completed in {search_time:.3f}s (total: {total_time:.3f}s)")
        
        return {
            "user_documents": user_docs,
            "admin_documents": admin_docs
        }
        
    except Exception as e:
        safe_log_error(f"Error in find_documents_parallel: {e}", exc_info=True)
        return {
            "user_documents": [],
            "admin_documents": []
        }

@tool
def find_document_from_user(search_query: str, uploader_username: str) -> list[dict]:
    """
    TÃ¬m kiáº¿m tÃ i liá»‡u tá»« ngÆ°á»i dÃ¹ng
    """
    safe_log_info("--- Running Tool: find_document_from_user ---")
    safe_log_info(f"Searching user documents for query: '{search_query}' and uploader: '{uploader_username}'")
    
    try:
        query_vector = get_embedding(search_query)
        if not query_vector:
            logger.error("Failed to generate embedding for search query")
            return []
            
        documents = find_similar_documents_hybrid_search(query_vector, search_query, uploader_username)
        safe_log_info(f"Found {len(documents)} user documents")
        
        return documents
    except Exception as e:
        safe_log_error(f"Error in find_document_from_user: {e}", exc_info=True)
        return []
    
@tool
def find_document_from_admin(search_query: str, uploader_username: str) -> list[dict]:
    """
    TÃ¬m kiáº¿m tÃ i liá»‡u tá»« admin
    """
    safe_log_info("--- Running Tool: find_document_from_admin ---")
    safe_log_info(f"Searching admin documents for query: '{search_query}' (uploader_username parameter ignored for admin search)")
    
    try:
        query_vector = get_embedding(search_query)
        if not query_vector:
            logger.error("Failed to generate embedding for search query")
            return []
            
        documents = find_similar_documents_vector_search(query_vector)
        safe_log_info(f"Found {len(documents)} admin documents")
        
        return documents
    except Exception as e:
        safe_log_error(f"Error in find_document_from_admin: {e}", exc_info=True)
        return []  
      
@tool
def rerank_documents(user_question: str, documents: list[dict]) -> list[dict]:
    """
    Rerank documents based on relevance to user question using Gemini API.
    """
    if not documents:
        return []

    docs_for_prompt = []
    for doc in documents:
        doc['_id'] = str(doc['_id'])
        docs_for_prompt.append({
            "id": doc['_id'],
            "content": doc.get('content', '')
        })

    prompt = f"""
    ### VAI TRÃ’ VÃ€ NHIá»†M Vá»¤ CHUYÃŠN SÃ‚U ###
    Báº¡n lÃ  má»™t há»‡ thá»‘ng PhÃ¢n loáº¡i vÃ  Xáº¿p háº¡ng Má»©c Ä‘á»™ LiÃªn quan (Relevance Classification and Ranking System) cá»±c ká»³ chÃ­nh xÃ¡c.
    Nhiá»‡m vá»¥ cá»§a báº¡n KHÃ”NG PHáº¢I lÃ  tráº£ lá»i cÃ¢u há»i. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  ÄÃNH GIÃ vÃ  CHáº¤M ÄIá»‚M tá»«ng tÃ i liá»‡u dá»±a trÃªn má»©c Ä‘á»™ chÃºng giÃºp tráº£ lá»i cÃ¢u há»i Ä‘Æ°á»£c cung cáº¥p.

    ### QUY TRÃŒNH SUY LUáº¬N (CHO Má»–I TÃ€I LIá»†U) ###
    1.  Äá»c ká»¹ vÃ  hiá»ƒu sÃ¢u [CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG].
    2.  Äá»c ká»¹ ná»™i dung cá»§a tÃ i liá»‡u Ä‘ang xÃ©t.
    3.  Tá»± Ä‘áº·t cÃ¢u há»i: "TÃ i liá»‡u nÃ y cÃ³ chá»©a thÃ´ng tin trá»±c tiáº¿p vÃ  Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khÃ´ng? Hay nÃ³ chá»‰ cung cáº¥p thÃ´ng tin ná»n táº£ng? Hay nÃ³ gáº§n nhÆ° khÃ´ng liÃªn quan?".
    4.  Dá»±a trÃªn cÃ¢u tráº£ lá»i, chá»n má»™t Ä‘iá»ƒm sá»‘ tá»« [Báº¢NG CHáº¤M ÄIá»‚M] dÆ°á»›i Ä‘Ã¢y.

    ### Báº¢NG CHáº¤M ÄIá»‚M CHI TIáº¾T (SCORING RUBRIC) ###
    - **1.0 (Ráº¥t cao):** TÃ i liá»‡u chá»©a cÃ¢u tráº£ lá»i trá»±c tiáº¿p, Ä‘áº§y Ä‘á»§ vÃ  rÃµ rÃ ng cho cÃ¢u há»i.
    - **0.7 (Cao):** TÃ i liá»‡u khÃ´ng tráº£ lá»i tháº³ng nhÆ°ng chá»©a thÃ´ng tin cá»‘t lÃµi, gáº§n nhÆ° khÃ´ng thá»ƒ thiáº¿u Ä‘á»ƒ suy ra cÃ¢u tráº£ lá»i.
    - **0.4 (Trung bÃ¬nh):** TÃ i liá»‡u cÃ³ liÃªn quan, Ä‘á» cáº­p Ä‘áº¿n cÃ¡c chá»§ Ä‘á» hoáº·c tá»« khÃ³a trong cÃ¢u há»i nhÆ°ng khÃ´ng Ä‘i vÃ o chi tiáº¿t hoáº·c khÃ´ng tráº£ lá»i trá»±c tiáº¿p.
    - **0.1 (Tháº¥p):** TÃ i liá»‡u cÃ³ váº» liÃªn quan á»Ÿ bá» máº·t (vÃ­ dá»¥: chung chá»§ Ä‘á») nhÆ°ng thá»±c cháº¥t khÃ´ng há»¯u Ã­ch Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    - **0.0 (KhÃ´ng liÃªn quan):** TÃ i liá»‡u nÃ³i vá» má»™t chá»§ Ä‘á» hoÃ n toÃ n khÃ¡c.

    ### QUY Táº®C Äá»ŠNH Dáº NG Äáº¦U RA (OUTPUT FORMAT RULES) ###
    - Káº¿t quáº£ Báº®T BUá»˜C pháº£i lÃ  má»™t chuá»—i JSON duy nháº¥t, lÃ  má»™t danh sÃ¡ch cÃ¡c object.
    - Má»—i object Báº®T BUá»˜C pháº£i cÃ³ hai key: "id" (dáº¡ng chuá»—i, láº¥y tá»« input), vÃ  "new_score" (dáº¡ng sá»‘ thá»±c).
    - TUYá»†T Äá»I KHÃ”NG thÃªm báº¥t ká»³ vÄƒn báº£n, ghi chÃº, hay lá»i giáº£i thÃ­ch nÃ o khÃ¡c. Chá»‰ tráº£ vá» chuá»—i JSON.

    ### VÃ Dá»¤ MáºªU (FEW-SHOT EXAMPLE) ###
    [VÃ Dá»¤ Äáº¦U VÃ€O]
    CÃ‚U Há»I:
    LÃ m tháº¿ nÃ o Ä‘á»ƒ táº¡o mÃ´i trÆ°á»ng áº£o trong Python?

    DANH SÃCH TÃ€I LIá»†U:
    [
      {{
        "id": "doc1",
        "uploader_username": "user1",
        "content": "Äá»ƒ táº¡o mÃ´i trÆ°á»ng áº£o, hÃ£y dÃ¹ng lá»‡nh `python -m venv myenv`."
      }},
      {{
        "id": "doc2",
        "uploader_username": "user2",
        "content": "Python lÃ  má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh phá»• biáº¿n."
      }}
    ]
    [VÃ Dá»¤ Káº¾T QUáº¢ JSON]
    [
      {{
        "id": "doc1",
        "uploader_username": "user1",
        "new_score": 1.0
      }},
      {{
        "id": "doc2",
        "uploader_username": "user2",
        "new_score": 0.1
      }}
    ]

    ---
    [Báº®T Äáº¦U Dá»® LIá»†U THá»°C Táº¾]

    ### Dá»® LIá»†U Äáº¦U VÃ€O ###
    [CÃ‚U Há»I]
    {user_question}

    [DANH SÃCH TÃ€I LIá»†U]
    {json.dumps(docs_for_prompt, ensure_ascii=False, indent=2)}

    ### Káº¾T QUáº¢ JSON ###
    """

    try:
        response = _get_model_client().models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=_generation_config
        )
        cleaned_response_text = response.text.strip().replace("```json", "").replace("```", "")
        rerank_results = json.loads(cleaned_response_text)

        scores_map = {item['id']: item['new_score'] for item in rerank_results}
        for doc in documents:
            doc['new_score'] = scores_map.get(doc['_id'], 0)

        reranked_documents = sorted(documents, key=lambda x: x['new_score'], reverse=True)

        return reranked_documents

    except Exception as e:
        safe_log_error(f"Error in rerank_documents: {e}")
        return documents

@tool
def classify_query_type(user_query: str) -> Dict[str, Any]:
    """
    PhÃ¢n loáº¡i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem cÃ³ cáº§n truy váº¥n kiáº¿n thá»©c hay khÃ´ng.
    Classifies user query to determine if knowledge retrieval is needed.
    
    Args:
        user_query: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng / User's question
        
    Returns:
        Dict with:
        - needs_retrieval (bool): True if query needs knowledge retrieval
        - query_type (str): Type of query (greeting, farewell, general_chat, knowledge_query)
        - confidence (float): Confidence score of classification
    """
    safe_log_info("--- Running Tool: classify_query_type ---")
    safe_log_info(f"Classifying query: '{user_query}'")

    try:
        # Normalize query for better matching
        normalized_query = user_query.lower().strip()
        
        # Enhanced patterns for better matching
        greeting_patterns = [
            r'^\s*(xin\s+chÃ o|chÃ o\s+báº¡n|chÃ o|xin\s+chao|chao)\s*[!.]*\s*$',
            r'^\s*(hi|hello|hey|good\s+morning|good\s+afternoon|good\s+evening)\s*[!.]*\s*$',
            r'^\s*(chÃ o\s+buá»•i\s+sÃ¡ng|chÃ o\s+buá»•i\s+chiá»u|chÃ o\s+buá»•i\s+tá»‘i)\s*[!.]*\s*$',
            r'^\s*(háº¿\s*lÃ´|há»ƒ\s*lÃ´|hÃªlÃ´|helo)\s*[!.]*\s*$',
            r'^\s*(alo|a\s*lo|alÃ´)\s*[!.]*\s*$'
        ]
        
        farewell_patterns = [
            r'^\s*(táº¡m\s+biá»‡t|tam\s+biet|goodbye|bye|see\s+you|háº¹n\s+gáº·p\s+láº¡i)\s*[!.]*\s*$',
            r'^\s*(chÃ o\s+táº¡m\s+biá»‡t|gáº·p\s+láº¡i\s+sau|bye\s+bye)\s*[!.]*\s*$',
            r'^\s*(cáº£m\s+Æ¡n\s+vÃ \s+táº¡m\s+biá»‡t|thanks\s+and\s+bye)\s*[!.]*\s*$'
        ]
        
        general_chat_patterns = [
            r'^\s*(báº¡n\s+khá»e\s+khÃ´ng|how\s+are\s+you|what\'s\s+up|whats\s+up)\s*[?!.]*\s*$',
            r'^\s*(báº¡n\s+tÃªn\s+gÃ¬|tÃªn\s+cá»§a\s+báº¡n|what\'s\s+your\s+name|whats\s+your\s+name)\s*[?!.]*\s*$',
            r'^\s*(cáº£m\s+Æ¡n|thank\s+you|thanks|thank)\s*[!.]*\s*$',
            r'^\s*(ok|okay|oke|Ä‘Æ°á»£c\s+rá»“i|tá»‘t)\s*[!.]*\s*$'
        ]

        # Check for greetings first (highest priority)
        for pattern in greeting_patterns:
            if re.search(pattern, normalized_query):
                safe_log_info(f"Query '{user_query}' classified as GREETING with high confidence")
                return {
                    "needs_retrieval": False,
                    "query_type": "greeting",
                    "confidence": 0.95
                }

        # Check for farewells
        for pattern in farewell_patterns:
            if re.search(pattern, normalized_query):
                safe_log_info(f"Query '{user_query}' classified as FAREWELL with high confidence")
                return {
                    "needs_retrieval": False,
                    "query_type": "farewell",
                    "confidence": 0.95
                }

        # Check for general chat
        for pattern in general_chat_patterns:
            if re.search(pattern, normalized_query):
                safe_log_info(f"Query '{user_query}' classified as GENERAL_CHAT with high confidence")
                return {
                    "needs_retrieval": False,
                    "query_type": "general_chat",
                    "confidence": 0.90
                }

        # Additional simple checks for very short queries
        if len(normalized_query) <= 3:
            common_short_greetings = ['hi', 'hey', 'yo', 'chÃ o', 'xin chÃ o']
            if normalized_query in common_short_greetings:
                safe_log_info(f"Query '{user_query}' classified as SHORT GREETING")
                return {
                    "needs_retrieval": False,
                    "query_type": "greeting",
                    "confidence": 0.90
                }

        # For more complex classification, use Gemini API
        safe_log_info(f"Using Gemini API for complex classification of: '{user_query}'")
        
        prompt = f"""
        PhÃ¢n tÃ­ch cÃ¢u há»i sau vÃ  xÃ¡c Ä‘á»‹nh xem cÃ³ cáº§n truy váº¥n kiáº¿n thá»©c Ä‘á»ƒ tráº£ lá»i khÃ´ng:

        CÃ¢u há»i: "{user_query}"

        HÃ£y phÃ¢n loáº¡i thÃ nh má»™t trong cÃ¡c dáº¡ng sau:
        1. **greeting** - CÃ¢u chÃ o há»i Ä‘Æ¡n giáº£n (xin chÃ o, hi, hello, chÃ o báº¡n, v.v.)
        2. **farewell** - CÃ¢u táº¡m biá»‡t (táº¡m biá»‡t, bye, goodbye, v.v.)
        3. **general_chat** - TrÃ² chuyá»‡n chung (há»i thÄƒm sá»©c khá»e, cáº£m Æ¡n, v.v.)
        4. **knowledge_query** - CÃ¢u há»i cáº§n kiáº¿n thá»©c cá»¥ thá»ƒ

        QUY Táº®C QUAN TRá»ŒNG:
        - Náº¿u lÃ  cÃ¢u chÃ o há»i Ä‘Æ¡n giáº£n â†’ needs_retrieval = false
        - Náº¿u lÃ  cÃ¢u táº¡m biá»‡t â†’ needs_retrieval = false  
        - Náº¿u lÃ  trÃ² chuyá»‡n chung â†’ needs_retrieval = false
        - Chá»‰ khi nÃ o thá»±c sá»± cáº§n thÃ´ng tin cá»¥ thá»ƒ â†’ needs_retrieval = true

        Tráº£ vá» káº¿t quáº£ dÆ°á»›i dáº¡ng JSON chÃ­nh xÃ¡c:
        {{
            "needs_retrieval": boolean,
            "query_type": string,
            "confidence": float
        }}
        """

        response = _get_model_client().models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=_generation_config
        )
        result_text = response.text.strip()
        
        # Clean up response text
        result_text = result_text.replace("```json", "").replace("```", "").strip()
        
        try:
            result = json.loads(result_text)
            safe_log_info(f"Gemini API classification result: {result}")
            
            # Validation
            if not isinstance(result.get('needs_retrieval'), bool):
                raise ValueError("needs_retrieval must be boolean")
            if result.get('query_type') not in ['greeting', 'farewell', 'general_chat', 'knowledge_query']:
                raise ValueError("Invalid query_type")
            if not isinstance(result.get('confidence'), (int, float)):
                raise ValueError("confidence must be numeric")
                
            return result
            
        except (json.JSONDecodeError, ValueError) as e:
            safe_log_error(f"Failed to parse Gemini API response: {e}. Raw response: {result_text}")
            # Fall back to knowledge query
            return {
                "needs_retrieval": True,
                "query_type": "knowledge_query",
                "confidence": 0.5
            }

    except Exception as e:
        safe_log_error(f"Error in classify_query_type: {e}")
        # Default to knowledge query if classification fails
        return {
            "needs_retrieval": True,
            "query_type": "knowledge_query",
            "confidence": 0.5
        }

@tool
def direct_response(user_query: str, query_type: str = "general_chat") -> str:
    """
    Tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng mÃ  khÃ´ng cáº§n truy váº¥n kiáº¿n thá»©c.
    Directly respond to user queries without knowledge retrieval.
    
    Args:
        user_query: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
        query_type: Loáº¡i cÃ¢u há»i (greeting, farewell, general_chat)
    """
    safe_log_info("--- Running Tool: direct_response ---")
    safe_log_info(f"Direct response to query type: '{query_type}' for query: '{user_query}'")
    
    try:
        # Provide quick responses for common patterns without API call
        if query_type == "greeting":
            import random
            greetings = [
                "Xin chÃ o! TÃ´i lÃ  WoxionChat AI, ráº¥t vui Ä‘Æ°á»£c gáº·p báº¡n! ğŸ‘‹\nHi! I'm WoxionChat AI, nice to meet you! ğŸ‘‹",
                "ChÃ o báº¡n! TÃ´i sáºµn sÃ ng há»— trá»£ báº¡n hÃ´m nay! ğŸ˜Š\nHello! I'm ready to help you today! ğŸ˜Š",
                "Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ AI WoxionChat. Báº¡n cáº§n tÃ´i há»— trá»£ gÃ¬ khÃ´ng? ğŸ¤–\nHi! I'm WoxionChat AI assistant. How can I help you? ğŸ¤–",
                "ChÃ o báº¡n! Ráº¥t vui Ä‘Æ°á»£c trÃ² chuyá»‡n vá»›i báº¡n! ğŸŒŸ\nHello! Great to chat with you! ğŸŒŸ"
            ]
            return random.choice(greetings)
        
        elif query_type == "farewell":
            import random
            farewells = [
                "Táº¡m biá»‡t! Háº¹n gáº·p láº¡i báº¡n láº§n sau! ğŸ‘‹\nGoodbye! See you next time! ğŸ‘‹",
                "ChÃ o táº¡m biá»‡t! ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh! ğŸ˜Š\nFarewell! Have a great day! ğŸ˜Š",
                "Háº¹n gáº·p láº¡i! LuÃ´n sáºµn sÃ ng há»— trá»£ báº¡n! ğŸ¤—\nSee you later! Always ready to help! ğŸ¤—",
                "Táº¡m biá»‡t! Cáº£m Æ¡n báº¡n Ä‘Ã£ trÃ² chuyá»‡n! ğŸ’«\nGoodbye! Thanks for chatting! ğŸ’«"
            ]
            return random.choice(farewells)
        
        elif query_type == "general_chat":
            # Handle common general chat patterns
            normalized_query = user_query.lower().strip()
            
            if "cáº£m Æ¡n" in normalized_query or "thank" in normalized_query:
                return "KhÃ´ng cÃ³ gÃ¬! TÃ´i ráº¥t vui Ä‘Æ°á»£c giÃºp Ä‘á»¡ báº¡n! ğŸ˜Š\nYou're welcome! I'm happy to help! ğŸ˜Š"
            
            if "khá»e" in normalized_query or "how are you" in normalized_query:
                return "TÃ´i ráº¥t khá»e vÃ  sáºµn sÃ ng há»— trá»£ báº¡n! CÃ²n báº¡n thÃ¬ sao? ğŸ˜Š\nI'm doing great and ready to help! How about you? ğŸ˜Š"
            
            if "tÃªn" in normalized_query or "name" in normalized_query:
                return "TÃ´i lÃ  WoxionChat AI, trá»£ lÃ½ thÃ´ng minh cá»§a báº¡n! ğŸ¤–\nI'm WoxionChat AI, your intelligent assistant! ğŸ¤–"
                
            if "ok" in normalized_query or "okay" in normalized_query or "Ä‘Æ°á»£c rá»“i" in normalized_query:
                return "Tá»‘t! TÃ´i sáºµn sÃ ng há»— trá»£ báº¡n tiáº¿p! ğŸ‘\nGreat! I'm ready to help you further! ğŸ‘"
        
        # For more complex responses, use Gemini API
        safe_log_info(f"Using Gemini API for direct response to query type: '{query_type}'")
        
        prompt = f"""
        Báº¡n lÃ  WoxionChat AI, má»™t trá»£ lÃ½ thÃ´ng minh, thÃ¢n thiá»‡n vÃ  chuyÃªn nghiá»‡p.
        HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch tá»± nhiÃªn vÃ  phÃ¹ há»£p.

        Loáº¡i cÃ¢u há»i: {query_type}
        CÃ¢u há»i/Tin nháº¯n: "{user_query}"

        QUY Táº®C TRáº¢I NGHIá»†M:
        1. Tráº£ lá»i ngáº¯n gá»n, tá»± nhiÃªn nhÆ° má»™t ngÆ°á»i báº¡n thÃ¢n thiá»‡n
        2. PhÃ¹ há»£p vá»›i loáº¡i cÃ¢u há»i (chÃ o há»i, táº¡m biá»‡t, trÃ² chuyá»‡n)
        3. LuÃ´n giá»¯ thÃ¡i Ä‘á»™ tÃ­ch cá»±c, chuyÃªn nghiá»‡p
        4. Sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ táº¡o cáº£m giÃ¡c thÃ¢n thiá»‡n
        5. Tráº£ lá»i báº±ng cáº£ tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh (Vietnamese first, English second)
        6. KHÃ”NG Ä‘Æ°a ra cÃ¢u tráº£ lá»i dáº¡ng JSON hoáº·c code
        7. KHÃ”NG há»i thÃ´ng tin cÃ¡ nhÃ¢n hoáº·c yÃªu cáº§u Ä‘Äƒng nháº­p
        8. Táº­p trung vÃ o viá»‡c táº¡o ra má»™t cuá»™c trÃ² chuyá»‡n tá»± nhiÃªn

        PHONG CÃCH:
        - Náº¿u greeting: ChÃ o Ä‘Ã³n nhiá»‡t tÃ¬nh, giá»›i thiá»‡u báº£n thÃ¢n
        - Náº¿u farewell: Táº¡m biá»‡t áº¥m Ã¡p, má»i quay láº¡i
        - Náº¿u general_chat: Tráº£ lá»i thÃ¢n thiá»‡n, tá»± nhiÃªn

        HÃ£y tráº£ lá»i má»™t cÃ¡ch tá»± nhiÃªn nháº¥t cÃ³ thá»ƒ!
        """

        response = _get_model_client().models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=_generation_config
        )
        result = response.text.strip()
        
        # Clean up response
        result = result.replace("```", "").replace("json", "").strip()
        safe_log_info(f"Generated direct response: {result[:100]}...")
        
        return result

    except Exception as e:
        safe_log_error(f"Error in direct_response: {e}")
        # Fallback responses
        if query_type == "greeting":
            return "Xin chÃ o! TÃ´i lÃ  WoxionChat AI, ráº¥t vui Ä‘Æ°á»£c gáº·p báº¡n! ğŸ‘‹\nHi! I'm WoxionChat AI, nice to meet you! ğŸ‘‹"
        elif query_type == "farewell":
            return "Táº¡m biá»‡t! Háº¹n gáº·p láº¡i báº¡n nhÃ©! ğŸ‘‹\nGoodbye! See you again! ğŸ‘‹"
        else:
            return "Ráº¥t vui Ä‘Æ°á»£c trÃ² chuyá»‡n vá»›i báº¡n! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n khÃ´ng? ğŸ˜Š\nIt's nice chatting with you! How can I help you? ğŸ˜Š"
